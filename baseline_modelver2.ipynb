{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from tqdm import tqdm  \n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import os\n",
        "import pickle\n",
        "import clip\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from inference.models.grasp_model import GraspModel, ResidualBlock\n",
        "from shapely.geometry import Polygon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the image with the grasping rectangle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hRAZ0TtSaPoC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2.4180e-01, 3.1207e+02, 1.7927e+02, 1.6738e+02, 3.7112e+01, 1.8286e+01])\n",
            "tensor([2.2193e-01, 2.7735e+02, 1.7888e+02, 1.8342e+02, 4.3059e+01, 1.6992e+02])\n",
            "tensor([9.5972e-02, 3.1199e+02, 1.8036e+02, 1.7114e+02, 4.8742e+01, 1.6855e+02])\n",
            "tensor([6.2602e-02, 2.7784e+02, 1.7790e+02, 1.8278e+02, 3.9815e+01, 1.8559e+01])\n",
            "tensor([4.8610e-02, 2.4890e+02, 1.5289e+02, 1.6061e+02, 3.7830e+01, 1.7023e+02])\n",
            "tensor([4.2725e-02, 2.5170e+02, 1.7719e+02, 1.6696e+02, 3.6233e+01, 1.9253e+01])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_12462/309958949.py:20: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
            "  box = np.int0(box)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def visualize_grasp(image_path, grasp_file_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Load grasp data using PyTorch\n",
        "    grasp_data = torch.load(grasp_file_path)\n",
        "\n",
        "    for grasp_instance in grasp_data:\n",
        "        print(grasp_instance)\n",
        "        # Extract parameters from each grasp instance\n",
        "        grasp_confidence, grasp_center_x, grasp_center_y, grasp_w, grasp_h, grasp_theta = grasp_instance.tolist()\n",
        "\n",
        "        # Create a rotated rectangle using the calculated points\n",
        "        rect = ((grasp_center_x, grasp_center_y), (grasp_w, grasp_h), grasp_theta)\n",
        "        box = cv2.boxPoints(rect)\n",
        "        box = np.int0(box)\n",
        "\n",
        "        # Draw the rectangle on the image\n",
        "        cv2.drawContours(image, [box], 0, (0, 255, 0), 2)  # Green rectangle\n",
        "\n",
        "    # Display the image with the grasp rectangles\n",
        "    cv2.imshow(\"Image with Grasp\", image)\n",
        "\n",
        "    # Check for the window to be closed by the user\n",
        "    while cv2.getWindowProperty(\"Image with Grasp\", cv2.WND_PROP_VISIBLE) >= 1:\n",
        "        key = cv2.waitKey(50)\n",
        "        if key == 27:  # Press ESC to exit\n",
        "            break\n",
        "\n",
        "    # Close the OpenCV window\n",
        "    cv2.destroyAllWindows()\n",
        "# Example usage:\n",
        "image_path = \"/home/ericnguyen/eric/master_thesis/src/grasp-anything/new_dataset/positive_images/0a35d0fd13323ae715e670a5e5074f9b07b8d2f4e70a1a852fbf1814d728aff6.jpg\"\n",
        "grasp_file_path = \"/home/ericnguyen/eric/master_thesis/src/grasp-anything/new_dataset/positive_grasp/0a35d0fd13323ae715e670a5e5074f9b07b8d2f4e70a1a852fbf1814d728aff6_0.pt\"\n",
        "\n",
        "visualize_grasp(image_path, grasp_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tensor values seem to represent the following grasp parameters:\n",
        "\n",
        "Grasp confidence: 3.9785e-02\n",
        "Grasp center (x, y): (79.639, 337.45)\n",
        "Grasp w: 205.54\n",
        "Grasp h: 39.499\n",
        "Grasp theta: 19.472"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q-mfi306aPoE"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# path of  mask files\n",
        "mask_file_path = '/home/ericnguyen/eric/master_thesis/src/grasp-anything/mask/0a35d0fd13323ae715e670a5e5074f9b07b8d2f4e70a1a852fbf1814d728aff6_0.npy'\n",
        "\n",
        "# Load the mask using NumPy\n",
        "mask = np.load(mask_file_path)\n",
        "\n",
        "# Create a figure and plot the mask\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(mask, cmap='viridis')  # Assuming the mask is a binary image\n",
        "\n",
        "# Find the coordinates of non-zero pixels in the mask\n",
        "y, x = np.nonzero(mask)\n",
        "\n",
        "# Create rectangles around the non-zero pixels in the mask\n",
        "for i in range(len(x)):\n",
        "    rect = patches.Rectangle((x[i], y[i]), 1, 1, linewidth=1, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "# Show the mask with grasp rectangles\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After creating new dataset with the same number of items in each folders, check the name is matching in each folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of matching items: 1976\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def count_matching_items(positive_grasp_folder, positive_image_folder, positive_prompt_folder):\n",
        "    grasp_files = os.listdir(positive_grasp_folder)\n",
        "    image_files = os.listdir(positive_image_folder)\n",
        "    prompt_files = os.listdir(positive_prompt_folder)\n",
        "\n",
        "    matching_count = 0\n",
        "\n",
        "    for grasp_file in grasp_files:\n",
        "        # Extract the base name without extension and \"_0\"\n",
        "        base_name = os.path.splitext(grasp_file)[0][:-2]\n",
        "\n",
        "        # Check if corresponding image and prompt files exist\n",
        "        if f\"{base_name}.jpg\" in image_files and f\"{base_name}.pkl\" in prompt_files:\n",
        "            matching_count += 1\n",
        "\n",
        "    return matching_count\n",
        "\n",
        "# Example usage:\n",
        "positive_grasp_folder = \"/home/ericnguyen/eric/master_thesis/src/grasp-anything/new_dataset/positive_grasp\"\n",
        "positive_image_folder = \"/home/ericnguyen/eric/master_thesis/src/grasp-anything/new_dataset/positive_images\"\n",
        "positive_prompt_folder = \"/home/ericnguyen/eric/master_thesis/src/grasp-anything/new_dataset/positive_prompt\"\n",
        "\n",
        "matching_count = count_matching_items(positive_grasp_folder, positive_image_folder, positive_prompt_folder)\n",
        "print(f\"Number of matching items: {matching_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All files have valid positive_grasp data.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the path to your Grasp-Anything dataset\n",
        "dataset_path = \"/home/ericnguyen/eric/master_thesis/src/grasp-anything/new_dataset\"\n",
        "\n",
        "def check_none_files(dataset_path):\n",
        "    none_files = []\n",
        "\n",
        "    # Assuming positive_grasp files are stored in the positive_grasp directory\n",
        "    positive_grasp_dir = os.path.join(dataset_path, \"positive_grasp\")\n",
        "\n",
        "    # Iterate through files in positive_grasp directory\n",
        "    for file in os.listdir(positive_grasp_dir):\n",
        "        file_path = os.path.join(positive_grasp_dir, file)\n",
        "\n",
        "        try:\n",
        "            positive_grasp = torch.load(file_path)\n",
        "            if positive_grasp is None:\n",
        "                none_files.append(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading positive_grasp data at {file_path}: {e}\")\n",
        "            none_files.append(file)\n",
        "\n",
        "    if none_files:\n",
        "        print(\"Files with None positive_grasp:\")\n",
        "        for file in none_files:\n",
        "            print(file)\n",
        "    else:\n",
        "        print(\"All files have valid positive_grasp data.\")\n",
        "\n",
        "# Call the function with your dataset_path\n",
        "check_none_files(dataset_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HVgHsf6aPoJ"
      },
      "source": [
        "***PROCESSING DATASET***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "g4WwfKk7aPoK"
      },
      "outputs": [],
      "source": [
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Image preprocessing\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.convert(\"RGB\")  # Ensure RGB format\n",
        "    image = image.resize((224, 224))  # Resize to match CLIP's input size\n",
        "    image = preprocess(image).unsqueeze(0).to(device)  # Preprocess for CLIP\n",
        "    return image\n",
        "\n",
        "# Text preprocessing\n",
        "def preprocess_text(text):\n",
        "    tokenized_text = clip.tokenize([text]).to(device)\n",
        "    return tokenized_text\n",
        "\n",
        "# Define the path to your Grasp-Anything dataset\n",
        "dataset_path = \"/home/ericnguyen/eric/master_thesis/src/grasp-anything/new_dataset\"\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Define dataset class\n",
        "class GraspDataset(Dataset):\n",
        "    def __init__(self, dataset_path):\n",
        "        self.image_paths = [os.path.join(dataset_path, \"positive_images\", file) for file in os.listdir(os.path.join(dataset_path, \"positive_images\"))]\n",
        "        self.text_descriptions = [self.load_text_description(file) for file in os.listdir(os.path.join(dataset_path, \"positive_prompt\"))]\n",
        "        self.positive_grasp_paths = [self.get_positive_grasp_path(file) for file in os.listdir(os.path.join(dataset_path, \"positive_images\"))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        text_description = self.text_descriptions[idx]\n",
        "        positive_grasp = torch.load(self.positive_grasp_paths[idx])\n",
        "\n",
        "        image = preprocess_image(image_path)\n",
        "        text_tokens = preprocess_text(text_description)\n",
        "\n",
        "        # max_size = max(grasp.size(0) for grasp in positive_grasp)\n",
        "        # padded_positive_grasp = torch.zeros(len(positive_grasp), max_size, positive_grasp[0].size(1))\n",
        "\n",
        "        # for i, grasp in enumerate(positive_grasp):\n",
        "        #     padded_positive_grasp[i, :grasp.size(0), :] = grasp\n",
        "\n",
        "        print(image.size(), text_tokens.size(), positive_grasp.size())\n",
        "        return image, text_tokens, positive_grasp\n",
        "    \n",
        "\n",
        "    def load_text_description(self, file):\n",
        "        prompt_path = os.path.join(dataset_path, \"positive_prompt\", file.replace(\".jpg\", \".pkl\"))\n",
        "        # Load and return the text description from the prompt file\n",
        "        with open(prompt_path, 'rb') as f:\n",
        "            text_description_tuple = pickle.load(f)\n",
        "\n",
        "        if isinstance(text_description_tuple, tuple) and len(text_description_tuple) == 2:\n",
        "            main_text, additional_info_list = text_description_tuple\n",
        "\n",
        "            # Only keep the specified parts of the prompt\n",
        "            additional_info_text = ' '.join(map(str, additional_info_list))\n",
        "\n",
        "            # Modify this line to include only the desired parts\n",
        "            text_description = additional_info_text\n",
        "        elif isinstance(text_description_tuple, str):\n",
        "            text_description = text_description_tuple\n",
        "        else:\n",
        "            raise TypeError(f\"Unexpected data type for text description in {prompt_path}\")\n",
        "\n",
        "        return text_description\n",
        "\n",
        "    def get_positive_grasp_path(self, file):\n",
        "        base_name = os.path.splitext(file)[0] + \"_0\"\n",
        "        return os.path.join(dataset_path, \"positive_grasp\", base_name + \".pt\")\n",
        "\n",
        "\n",
        "# New class that inherits from GraspDataset\n",
        "class GraspSubset(GraspDataset):\n",
        "    def __init__(self, dataset, indices):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[self.indices[idx]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "\n",
        "# Create dataset and split into train, test, and validation sets\n",
        "grasp_dataset = GraspDataset(dataset_path)\n",
        "dataset_size = len(grasp_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "test_size = (dataset_size - train_size) // 2\n",
        "val_size = dataset_size - train_size - test_size\n",
        "\n",
        "train_indices, test_indices, val_indices = random_split(list(range(dataset_size)), [train_size, test_size, val_size])\n",
        "\n",
        "# Create data loaders for train, test, and validation sets\n",
        "train_dataset = GraspSubset(grasp_dataset, train_indices)\n",
        "test_dataset = GraspSubset(grasp_dataset, test_indices)\n",
        "val_dataset = GraspSubset(grasp_dataset, val_indices)\n",
        "\n",
        "# Define sub-batch size\n",
        "sub_batch_size = batch_size // 2\n",
        "\n",
        "# Create data loaders with sub-batch strategy\n",
        "train_loader = DataLoader(train_dataset, batch_size=sub_batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=sub_batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=sub_batch_size, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checking the input tensor size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 224, 224]) torch.Size([1, 77]) torch.Size([1, 6])\n",
            "torch.Size([1, 3, 224, 224]) torch.Size([1, 77]) torch.Size([3, 6])\n",
            "torch.Size([1, 3, 224, 224]) torch.Size([1, 77]) torch.Size([11, 6])\n",
            "torch.Size([1, 3, 224, 224]) torch.Size([1, 77]) torch.Size([7, 6])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [1, 6] at entry 0 and [3, 6] at entry 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m image_input, text_tokens_input, positive_grasp \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         \u001b[39m# Print the sizes of input tensors\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Image size: \u001b[39m\u001b[39m{\u001b[39;00mimage_input\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m, Text size: \u001b[39m\u001b[39m{\u001b[39;00mtext_tokens_input\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m, Grasp size: \u001b[39m\u001b[39m{\u001b[39;00mpositive_grasp\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 6] at entry 0 and [3, 6] at entry 1"
          ]
        }
      ],
      "source": [
        "for image_input, text_tokens_input, positive_grasp in train_loader:\n",
        "        # Print the sizes of input tensors\n",
        "        print(f\" Image size: {image_input.size()}, Text size: {text_tokens_input.size()}, Grasp size: {positive_grasp.size()}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVfuHG5OaPoM"
      },
      "source": [
        "***ARCHITECHTURE***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "drol8G2aaPoM"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MultiModalGenerativeResnet(GraspModel):\n",
        "    def __init__(self, input_channels=4, text_feature_size=64, output_channels=1, channel_size=32, dropout=False, prob=0.0):\n",
        "        super(MultiModalGenerativeResnet, self).__init__()\n",
        "\n",
        "        # Image pathway\n",
        "        self.image_conv1 = nn.Conv2d(input_channels, channel_size, kernel_size=9, stride=1, padding=4)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_size)\n",
        "\n",
        "        # Common layers after merging\n",
        "        self.merge_fc = nn.Linear(text_feature_size + (channel_size * 8 * 8), channel_size * 4)  # Combined features\n",
        "\n",
        "        # Rest of the Antipodal model\n",
        "        self.res1 = ResidualBlock(channel_size * 4, channel_size * 4)\n",
        "        self.res2 = ResidualBlock(channel_size * 4, channel_size * 4)\n",
        "        self.res3 = ResidualBlock(channel_size * 4, channel_size * 4)\n",
        "        self.res4 = ResidualBlock(channel_size * 4, channel_size * 4)\n",
        "        self.res5 = ResidualBlock(channel_size * 4, channel_size * 4)\n",
        "\n",
        "        self.conv4 = nn.ConvTranspose2d(channel_size * 4, channel_size * 2, kernel_size=4, stride=2, padding=1,\n",
        "                                        output_padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(channel_size * 2)\n",
        "\n",
        "        self.conv5 = nn.ConvTranspose2d(channel_size * 2, channel_size, kernel_size=4, stride=2, padding=2,\n",
        "                                        output_padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(channel_size)\n",
        "\n",
        "        self.conv6 = nn.ConvTranspose2d(channel_size, channel_size, kernel_size=9, stride=1, padding=4)\n",
        "\n",
        "        # Output layers for 5-dimensional grasp configuration\n",
        "        self.pos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)\n",
        "        self.height_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)\n",
        "        self.width_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)\n",
        "        self.theta_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        if self.dropout:\n",
        "            self.dropout_pos = nn.Dropout(p=prob)\n",
        "            self.dropout_height = nn.Dropout(p=prob)\n",
        "            self.dropout_width = nn.Dropout(p=prob)\n",
        "            self.dropout_theta = nn.Dropout(p=prob)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "\n",
        "    def forward(self, image, text_features):\n",
        "        # Image pathway\n",
        "        x = F.relu(self.bn1(self.image_conv1(image)))\n",
        "        # Flatten image features\n",
        "        x_image = x_image.view(x.size(0), -1)  # Flatten to match dimensions\n",
        "\n",
        "        # Flatten text features (assuming they're already in a suitable format)\n",
        "        text_features = text_features.view(text_features.size(0), -1)\n",
        "\n",
        "        # Combine image and text features\n",
        "        combined_features = torch.cat((x_image, text_features), dim=1)\n",
        "        combined_features = F.relu(self.merge_fc(combined_features))\n",
        "\n",
        "        # Continue with the rest of the Antipodal model\n",
        "        x = self.res1(combined_features)\n",
        "        x = self.res2(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.res5(x)\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = self.conv6(x)\n",
        "\n",
        "       # Output layers for 5-dimensional grasp configuration\n",
        "        if self.dropout:\n",
        "            pos_output = self.pos_output(self.dropout_pos(combined_features))\n",
        "            height_output = self.height_output(self.dropout_height(combined_features))\n",
        "            width_output = self.width_output(self.dropout_width(combined_features))\n",
        "            theta_output = self.theta_output(self.dropout_theta(combined_features))\n",
        "        else:\n",
        "            pos_output = self.pos_output(combined_features)\n",
        "            height_output = self.height_output(combined_features)\n",
        "            width_output = self.width_output(combined_features)\n",
        "            theta_output = self.theta_output(combined_features)\n",
        "\n",
        "        return pos_output, height_output, width_output, theta_output\n",
        "\n",
        "    def compute_loss(self, image, text, yc):\n",
        "        y_pos, y_height, y_width, y_theta = yc\n",
        "        pos_pred, height_pred, width_pred, theta_pred = self(image, text)\n",
        "\n",
        "        p_loss = F.smooth_l1_loss(pos_pred, y_pos)\n",
        "        h_loss = F.smooth_l1_loss(height_pred, y_height)\n",
        "        w_loss = F.smooth_l1_loss(width_pred, y_width)\n",
        "        theta_loss = F.smooth_l1_loss(theta_pred, y_theta)\n",
        "\n",
        "        return {\n",
        "            'loss': p_loss + h_loss + w_loss + theta_loss,\n",
        "            'losses': {\n",
        "                'p_loss': p_loss,\n",
        "                'h_loss': h_loss,\n",
        "                'w_loss': w_loss,\n",
        "                'theta_loss': theta_loss\n",
        "            },\n",
        "            'pred': {\n",
        "                'pos': pos_pred,\n",
        "                'height': height_pred,\n",
        "                'width': width_pred,\n",
        "                'theta': theta_pred\n",
        "            }\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eAm9DpMaPoN"
      },
      "source": [
        "***TRAINING FUNCTION***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "SQXohwM2aPoN"
      },
      "outputs": [],
      "source": [
        "def train(epoch, net, device, train_loader, optimizer, batches_per_epoch, vis=False):\n",
        "    results = {\n",
        "        'loss': 0,\n",
        "        'losses': {}\n",
        "    }\n",
        "\n",
        "    net.train()\n",
        "    batch_idx = 0\n",
        "    while batch_idx <= batches_per_epoch:\n",
        "        for image_input, text_tokens_input, positive_grasp in train_loader:\n",
        "        \n",
        "            # Print the sizes of input tensors\n",
        "            print(f\"Batch {batch_idx}: Image size: {image_input.size()}, Text size: {text_tokens_input.size()}, Grasp size: {positive_grasp.size()}\")\n",
        "            batch_idx += 1\n",
        "            if batch_idx >= batches_per_epoch:\n",
        "                break\n",
        "            \n",
        "            # encode the image features as the inputs\n",
        "            image_features = model.encode_image(image_input)\n",
        "            text_features = model.encode_text(text_tokens_input)\n",
        "            image = image_features.to(device)\n",
        "            text_tokens = text_features.to(device)\n",
        "\n",
        "            grasp = [yy.to(device) for yy in positive_grasp]\n",
        "            lossd = net.compute_loss(image, text_tokens, grasp)\n",
        "\n",
        "            loss = lossd['loss']\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
        "\n",
        "            results['loss'] += loss.item()\n",
        "            for ln, l in lossd['losses'].items():\n",
        "                if ln not in results['losses']:\n",
        "                    results['losses'][ln] = 0\n",
        "                results['losses'][ln] += l.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Display the images\n",
        "            # if vis:\n",
        "            #     imgs = []\n",
        "            #     n_img = min(4, image_input.shape[0])\n",
        "            #     for idx in range(n_img):\n",
        "            #         imgs.extend([image_input[idx,].numpy().squeeze()] + [yi[idx,].numpy().squeeze() for yi in positive_grasp] + [\n",
        "            #             image_input[idx,].numpy().squeeze()] + [pc[idx,].detach().cpu().numpy().squeeze() for pc in\n",
        "            #                                           lossd['pred'].values()])\n",
        "            #     gridshow('Display', imgs,\n",
        "            #              [(image.min().item(), image.max().item()), (0.0, 1.0), (0.0, 1.0), (-1.0, 1.0),\n",
        "            #               (0.0, 1.0)] * 2 * n_img,\n",
        "            #              [cv2.COLORMAP_BONE] * 10 * n_img, 10)\n",
        "            #     cv2.waitKey(2)\n",
        "\n",
        "\n",
        "    results['loss'] /= batch_idx\n",
        "    for key in results['losses']:\n",
        "        results['losses'][key] /= batch_idx\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***EVALUATE FUNCTION***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def evaluate(net, device, data_loader):\n",
        "    net.eval()\n",
        "\n",
        "    iou_threshold = 0.25  # You can adjust this threshold based on your task\n",
        "\n",
        "    total_iou = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image_input, text_tokens_input, positive_grasp in data_loader:\n",
        "            image_features = model.encode_image(image_input)\n",
        "            text_features = model.encode_text(text_tokens_input)\n",
        "            image = image_features.to(device)\n",
        "            text_tokens = text_features.to(device)\n",
        "\n",
        "            grasp = [yy.to(device) for yy in positive_grasp]\n",
        "\n",
        "            # Get predicted grasp parameters\n",
        "            _, _, _, theta_pred = net(image, text_tokens)['pred'].values()\n",
        "\n",
        "            # Convert grasp parameters to bounding boxes\n",
        "            pred_bboxes = grasps_to_bboxes(theta_pred)\n",
        "\n",
        "            # Convert ground truth grasp parameters to bounding boxes\n",
        "            gt_bboxes = grasps_to_bboxes(grasp[0])\n",
        "\n",
        "            # Compute IoU for each sample in the batch\n",
        "            batch_iou = calculate_iou(pred_bboxes, gt_bboxes)\n",
        "\n",
        "            total_iou += batch_iou.sum().item()\n",
        "            total_samples += len(batch_iou)\n",
        "\n",
        "    average_iou = total_iou / total_samples\n",
        "\n",
        "    return average_iou\n",
        "\n",
        "def grasps_to_bboxes(grasps):\n",
        "    # convert grasp representation to bbox\n",
        "    x = grasps[:, 0]\n",
        "    y = grasps[:, 1]\n",
        "    theta = grasps[:, 2]\n",
        "    h = grasps[:, 3]\n",
        "    w = grasps[:, 4]\n",
        "    x1 = x - w/2 * torch.cos(theta) + h/2 * torch.sin(theta)\n",
        "    y1 = y - w/2 * torch.sin(theta) - h/2 * torch.cos(theta)\n",
        "    x2 = x + w/2 * torch.cos(theta) + h/2 * torch.sin(theta)\n",
        "    y2 = y + w/2 * torch.sin(theta) - h/2 * torch.cos(theta)\n",
        "    x3 = x + w/2 * torch.cos(theta) - h/2 * torch.sin(theta)\n",
        "    y3 = y + w/2 * torch.sin(theta) + h/2 * torch.cos(theta)\n",
        "    x4 = x - w/2 * torch.cos(theta) - h/2 * torch.sin(theta)\n",
        "    y4 = y - w/2 * torch.sin(theta) + h/2 * torch.cos(theta)\n",
        "    bboxes = torch.stack((x1, y1, x2, y2, x3, y3, x4, y4), 1)\n",
        "    return bboxes\n",
        "\n",
        "def calculate_iou(pred_bboxes, gt_bboxes):\n",
        "    \"\"\"\n",
        "    Calculate Intersection over Union (IoU) between predicted and ground truth bounding boxes.\n",
        "    \"\"\"\n",
        "    iou_list = []\n",
        "    for pred_bbox, gt_bbox in zip(pred_bboxes, gt_bboxes):\n",
        "        iou = box_iou(pred_bbox, gt_bbox)\n",
        "        iou_list.append(iou.item())\n",
        "    return torch.tensor(iou_list)\n",
        "\n",
        "def box_iou(bbox_value, bbox_target):\n",
        "    p1 = Polygon(bbox_value.view(-1, 2).tolist())\n",
        "    p2 = Polygon(bbox_target.view(-1, 2).tolist())\n",
        "    iou = p1.intersection(p2).area / (p1.area + p2.area - p1.intersection(p2).area)\n",
        "    return iou\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***TRAINING MODEL***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [3, 6] at entry 0 and [1, 6] at entry 2",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m train_results \u001b[39m=\u001b[39m train(epoch, model, device, train_loader, optimizer, batches_per_epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Evaluation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m average_iou \u001b[39m=\u001b[39m evaluate(model, device, val_loader)\n",
            "\u001b[1;32m/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m batch_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwhile\u001b[39;00m batch_idx \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m batches_per_epoch:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# for image_input, text_tokens_input, positive_grasp in train_loader:\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (image_input, text_tokens_input, positive_grasp) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m             \u001b[39m# Print the sizes of input tensors\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ericnguyen/eric/master_thesis/baseline_model/baseline_modelver2.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch \u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m}\u001b[39;00m\u001b[39m: Image size: \u001b[39m\u001b[39m{\u001b[39;00mimage_input\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m, Text size: \u001b[39m\u001b[39m{\u001b[39;00mtext_tokens_input\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m, Grasp size: \u001b[39m\u001b[39m{\u001b[39;00mpositive_grasp\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 6] at entry 0 and [1, 6] at entry 2"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Define your model\n",
        "model = MultiModalGenerativeResnet()\n",
        "\n",
        "# Define optimizer and other training parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 5  # You can adjust this number based on your needs\n",
        "batches_per_epoch = len(train_loader)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    # Training\n",
        "    train_results = train(epoch, model, device, train_loader, optimizer, batches_per_epoch)\n",
        "\n",
        "    # Evaluation\n",
        "    average_iou = evaluate(model, device, val_loader)\n",
        "    print(f\"Average IoU on Validation Set: {average_iou:.4f}\")\n",
        "\n",
        "\n",
        "print(\"Training and Evaluation completed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
